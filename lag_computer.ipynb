{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16e6a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import csv\n",
    "from helpers import config\n",
    "from helpers.loading import load_daily_data ,file_exist\n",
    "from helpers.algorithm import find_best_delay\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43aaf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_dates(stock,signal):\n",
    "    \"\"\"return a sorted list of all dates were trades/bbo (signal) are available in the data\"\"\"\n",
    "    \n",
    "    def extract_date(s):\n",
    "        try:\n",
    "            date = re.search(r\"[0-9]{4}-[0-9]{2}-[0-9]{2}\",s).group(0)\n",
    "        except :\n",
    "            print(s)\n",
    "        return date\n",
    "    all_files = glob.glob(f\"./Data/{signal}/*/*\")\n",
    "    all_dates = [extract_date(s) for s in all_files] \n",
    "    all_dates = list(set(all_dates))\n",
    "    all_dates.sort()\n",
    "    return all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83041f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 dates to process\n"
     ]
    }
   ],
   "source": [
    "all_dates = get_all_dates(config[\"stock\"],config[\"signal\"])\n",
    "print(f\"{len(all_dates)} dates to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71b9306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing data : 2010-01-01 B00%\n",
      "missing data : 2010-01-01 C\n",
      "missing data : 2010-01-01 DF\n",
      "missing data : 2010-01-01 II\n",
      "missing data : 2010-01-01 MW\n",
      "missing data : 2010-01-01 O\n",
      "missing data : 2010-01-01 OQ\n",
      "missing data : 2010-01-01 P\n",
      "missing data : 2010-01-18 B215%\n",
      "missing data : 2010-01-18 C\n",
      "missing data : 2010-01-18 DF\n",
      "missing data : 2010-01-18 II\n",
      "missing data : 2010-01-18 MW\n",
      "missing data : 2010-01-18 O\n",
      "missing data : 2010-01-18 OQ\n",
      "missing data : 2010-01-18 P\n",
      "missing data : 2010-02-15 B.877%\n",
      "missing data : 2010-02-15 C\n",
      "missing data : 2010-02-15 DF\n",
      "missing data : 2010-02-15 II\n",
      "missing data : 2010-02-15 MW\n",
      "missing data : 2010-02-15 O\n",
      "missing data : 2010-02-15 OQ\n",
      "missing data : 2010-02-15 P\n",
      "10 dates processed in 264.95s59%\n"
     ]
    }
   ],
   "source": [
    "fieldnames = ['date', 'market1','market2',\"lag\"]\n",
    "\n",
    "results_path = config[\"files\"][\"results\"][\"all_best_lags\"] # file where to write the computed lags\n",
    "\n",
    "result_file_exists = file_exist(results_path) \n",
    "csvfile = open(results_path, 'a', newline='') \n",
    "writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "if result_file_exists:\n",
    "    processed_dates = set(pd.read_csv(results_path).date.unique())\n",
    "else: \n",
    "    # if the file is new, we need to write headers\n",
    "    writer.writeheader()\n",
    "    processed_dates = set()\n",
    "\n",
    "\n",
    "max_iterations = 10\n",
    "start_time = time.time()\n",
    "date_count = 0 # number of dates processed\n",
    "for date_id,date in enumerate(all_dates):\n",
    "    print(f\"data:{date}, {date_id}:{len(all_dates)}, {100*date_id/len(all_dates):0.3f}%\", end=\"\\r\")\n",
    "    try:\n",
    "        daily_data = load_daily_data(date)\n",
    "    except:\n",
    "        # in case all markets do not provide data for the given date, we skip the date\n",
    "        continue\n",
    "    \n",
    "    # we skip the current date if it has already been processed\n",
    "    if date in processed_dates:\n",
    "        continue\n",
    "        \n",
    "    for i,n1 in enumerate(daily_data):\n",
    "        for j,n2 in enumerate(daily_data):\n",
    "            if i>j: # avoid symetric (corr(a,b)=corr(b,a)) and meaningless (corr(a,a)=1) calculations\n",
    "                best_delay, delays, correlations, los, his = find_best_delay(daily_data,n1,n2,step_size=1000)\n",
    "                # write the computed result\n",
    "                writer.writerow({'date': date, 'market1': n1,'market2': n2,'lag': best_delay})\n",
    "                writer.writerow({'date': date, 'market1': n2,'market2': n1,'lag': -best_delay})\n",
    "    csvfile.flush() # flush every time we processed a date\n",
    "    date_count+=1\n",
    "    if date_count>=max_iterations:\n",
    "        break\n",
    "print(f\"{date_count} dates processed in {time.time()-start_time:0.2f}s\")\n",
    "csvfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
